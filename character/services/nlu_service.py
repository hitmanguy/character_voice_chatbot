import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from config import Config
from llm_integration import generate_llm_response
import logging

class NaturalLanguageUnderstandingService:
    def __init__(self, config: Config):
        self.config = config
        self.device = config.device
        
        logging.info("Loading translation model (NLLB) to CPU...")
        self.translator_tokenizer = AutoTokenizer.from_pretrained(config.translation_model_id)
        self.translator_model = AutoModelForSeq2SeqLM.from_pretrained(config.translation_model_id).to("cpu")
        logging.info("âœ… Translation model loaded.")

    def translate(self, text_to_translate: str, src_lang: str, tgt_lang: str) -> str:
        """
        Translates text between languages using the dedicated NLLB model.
        Assumes the input text is already in the correct native script.
        """
        logging.info(f"Translating from '{src_lang}' to '{tgt_lang}' with NLLB...")
        model_on_device = self.translator_model.to(self.device)
        
        src_nllb_code = self.config.ISO_TO_NLLB_MAPPING.get(src_lang)
        tgt_nllb_code = self.config.ISO_TO_NLLB_MAPPING.get(tgt_lang)

        if not src_nllb_code or not tgt_nllb_code:
            raise ValueError(f"Language code not supported by NLLB mapping: {src_lang} or {tgt_lang}")

        self.translator_tokenizer.src_lang = src_nllb_code
        
        try:
            inputs = self.translator_tokenizer(text_to_translate, return_tensors="pt").to(self.device)
            translated_tokens = model_on_device.generate(
                **inputs,
                forced_bos_token_id=self.translator_tokenizer.lang_code_to_id[tgt_nllb_code]
            )
            translated_text = self.translator_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
            logging.info(f"NLLB Translation result: '{translated_text}'")
            return translated_text
        finally:
            model_on_device.to("cpu")
            if self.device == "cuda": torch.cuda.empty_cache()

    def cleanup_character_response(self, character_text: str) -> str:
        """
        Uses Gemini to sanitize the English output from the character model.
        """
        logging.info("Sanitizing character response with Gemini...")
        prompt = f"""
        The following text was generated by a character AI. Clean it up into a single, coherent, in-character response.
        Remove any artifacts, repetitions, or sentences that seem out-of-character.

        Raw AI text: "{character_text}"

        Clean, final response:
        """
        final_text = generate_llm_response(prompt)
        logging.info(f"Sanitized English response: '{final_text}'")
        return final_text